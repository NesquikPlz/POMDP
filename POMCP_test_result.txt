(pybullet) C:\Users\ajy84\OneDrive\바탕 화면\Programming>C:/Users/ajy84/.conda/envs/pybullet/python.exe "c:/Users/ajy84/OneDrive/바탕 화면/Programming/POMDP/tiger_problem.py"
** Testing POMCP **
==== Step 1 ====
True state: tiger-right
Belief: [(State(tiger-left), 0.57), (State(tiger-right), 0.43)]
Action: listen
Reward: -1
>> Observation: tiger-right
Num sims: 1000
Plan time: 0.27989
==== Step 2 ====
True state: tiger-right
Belief: [(State(tiger-right), 0.8235294117647058), (State(tiger-left), 0.17647058823529413)]
Action: stay
Reward: -1
>> Observation: tiger-right
Num sims: 1000
Plan time: 0.21431
==== Step 3 ====
True state: tiger-right
Belief: [(State(tiger-right), 0.8337801608579088), (State(tiger-left), 0.16621983914209115)]
Action: listen
Reward: -1
>> Observation: tiger-right
Num sims: 1000
Plan time: 0.22771
==== Step 4 ====
True state: tiger-right
Belief: [(State(tiger-right), 0.9683908045977011), (State(tiger-left), 0.031609195402298854)]
Action: listen
Reward: -1
>> Observation: tiger-right
Num sims: 1000
Plan time: 0.21970
==== Step 5 ====
True state: tiger-right
Belief: [(State(tiger-right), 0.9873417721518988), (State(tiger-left), 0.012658227848101266)]
Action: stay
Reward: -1
>> Observation: tiger-right
Num sims: 1000
Plan time: 0.26248
==== Step 6 ====
True state: tiger-right
Belief: [(State(tiger-right), 0.9895287958115183), (State(tiger-left), 0.010471204188481676)]
Action: stay
Reward: -1
>> Observation: tiger-right
Num sims: 1000
Plan time: 0.23472
==== Step 7 ====
True state: tiger-right
Belief: [(State(tiger-right), 0.9933333333333333), (State(tiger-left), 0.006666666666666667)]
Action: stay
Reward: -1
>> Observation: tiger-right
Num sims: 1000
Plan time: 0.29224
==== Step 8 ====
True state: tiger-right
Belief: [(State(tiger-right), 1.0)]
Action: stay
Reward: -1
>> Observation: tiger-right
Num sims: 1000
Plan time: 0.24494
==== Step 9 ====
True state: tiger-right
Belief: [(State(tiger-right), 1.0)]
Action: listen
Reward: -1
>> Observation: tiger-right
Num sims: 1000
Plan time: 0.26157
==== Step 10 ====
True state: tiger-right
Belief: [(State(tiger-right), 1.0)]
Action: listen
Reward: -1
>> Observation: tiger-right
Num sims: 1000
Plan time: 0.19285
==== Step 11 ====
True state: tiger-right
Belief: [(State(tiger-right), 1.0)]
Action: listen
Reward: -1
>> Observation: tiger-right
Num sims: 1000
Plan time: 0.23383
==== Step 12 ====
True state: tiger-right
Belief: [(State(tiger-right), 1.0)]
Action: stay
Reward: -1
>> Observation: tiger-right
Num sims: 1000
Plan time: 0.22300
==== Step 13 ====
True state: tiger-right
Belief: [(State(tiger-right), 1.0)]
Action: listen
Reward: -1
>> Observation: tiger-right
Num sims: 1000
Plan time: 0.42531
==== Step 14 ====
True state: tiger-right
Belief: [(State(tiger-right), 1.0)]
Action: listen
Reward: -1
>> Observation: tiger-right
Num sims: 1000
Plan time: 0.28470
==== Step 15 ====
True state: tiger-right
Belief: [(State(tiger-right), 1.0)]
Action: stay
Reward: -1
>> Observation: tiger-right
Num sims: 1000
Plan time: 0.31192
==== Step 16 ====
True state: tiger-right
Belief: [(State(tiger-right), 1.0)]
Action: listen
Reward: -1
>> Observation: tiger-right
Num sims: 1000
Plan time: 0.23122
==== Step 17 ====
True state: tiger-right
Belief: [(State(tiger-right), 1.0)]
Action: listen
Reward: -1
>> Observation: tiger-right
Num sims: 1000
Plan time: 0.23397
==== Step 18 ====
True state: tiger-right
Belief: [(State(tiger-right), 1.0)]
Action: listen
Reward: -1
>> Observation: tiger-right
Num sims: 1000
Plan time: 0.45900
==== Step 19 ====
True state: tiger-right
Belief: [(State(tiger-right), 1.0)]
Action: stay
Reward: -1
>> Observation: tiger-right
Num sims: 1000
Plan time: 0.39664
==== Step 20 ====
True state: tiger-right
Belief: [(State(tiger-right), 1.0)]
Action: listen
Reward: -1
>> Observation: tiger-right
Num sims: 1000
Plan time: 0.57835
==== Step 21 ====
True state: tiger-right
Belief: [(State(tiger-right), 1.0)]
Action: listen
Reward: -1
>> Observation: tiger-right
Num sims: 1000
Plan time: 0.44475
==== Step 22 ====
True state: tiger-right
Belief: [(State(tiger-right), 1.0)]
Action: listen
Reward: -1
>> Observation: tiger-right
Num sims: 1000
Plan time: 0.25589
==== Step 23 ====
True state: tiger-right
Belief: [(State(tiger-right), 1.0)]
Action: stay
Reward: -1
>> Observation: tiger-right
Num sims: 1000
Plan time: 0.43810
==== Step 24 ====
True state: tiger-right
Belief: [(State(tiger-right), 1.0)]
Action: listen
Reward: -1
>> Observation: tiger-right
Num sims: 1000
Plan time: 0.41383
==== Step 25 ====
True state: tiger-right
Belief: [(State(tiger-right), 1.0)]
Action: listen
Reward: -1
>> Observation: tiger-right
Num sims: 1000
Plan time: 0.37298
==== Step 26 ====
True state: tiger-right
Belief: [(State(tiger-right), 1.0)]
Action: listen
Reward: -1
>> Observation: tiger-right
Num sims: 1000
Plan time: 0.42600
==== Step 27 ====
True state: tiger-right
Belief: [(State(tiger-right), 1.0)]
Action: stay
Reward: -1
>> Observation: tiger-right
Num sims: 1000
Plan time: 0.36132
==== Step 28 ====
True state: tiger-right
Belief: [(State(tiger-right), 1.0)]
Action: stay
Reward: -1
>> Observation: tiger-right
Num sims: 1000
Plan time: 0.28715
==== Step 29 ====
True state: tiger-right
Belief: [(State(tiger-right), 1.0)]
Action: listen
Reward: -1
>> Observation: tiger-right
Num sims: 1000
Plan time: 0.25561
==== Step 30 ====
True state: tiger-right
Belief: [(State(tiger-right), 1.0)]
Action: listen
Reward: -1
>> Observation: tiger-right
Num sims: 1000
Plan time: 0.40827
==== Step 31 ====
True state: tiger-right
Belief: [(State(tiger-right), 1.0)]
Action: listen
Reward: -1
>> Observation: tiger-right
Num sims: 1000
Plan time: 0.20575
==== Step 32 ====
True state: tiger-right
Belief: [(State(tiger-right), 1.0)]
Action: stay
Reward: -1
>> Observation: tiger-right
Num sims: 1000
Plan time: 0.17604
==== Step 33 ====
True state: tiger-right
Belief: [(State(tiger-right), 1.0)]
Action: listen
Reward: -1
>> Observation: tiger-right
Num sims: 1000
Plan time: 0.19734
==== Step 34 ====
True state: tiger-right
Belief: [(State(tiger-right), 1.0)]
Action: stay
Reward: -1
>> Observation: tiger-right
Num sims: 1000
Plan time: 0.29645
==== Step 35 ====
True state: tiger-right
Belief: [(State(tiger-right), 1.0)]
Action: stay
Reward: -1
>> Observation: tiger-right
Num sims: 1000
Plan time: 0.19183
==== Step 36 ====
True state: tiger-right
Belief: [(State(tiger-right), 1.0)]
Action: listen
Reward: -1
>> Observation: tiger-right
Num sims: 1000
Plan time: 0.20220
==== Step 37 ====
True state: tiger-right
Belief: [(State(tiger-right), 1.0)]
Action: stay
Reward: -1
>> Observation: tiger-right
Num sims: 1000
Plan time: 0.25676
==== Step 38 ====
True state: tiger-right
Belief: [(State(tiger-right), 1.0)]
Action: listen
Reward: -1
>> Observation: tiger-right
Num sims: 1000
Plan time: 0.27973
==== Step 39 ====
True state: tiger-right
Belief: [(State(tiger-right), 1.0)]
Action: listen
Reward: -1
>> Observation: tiger-right
Num sims: 1000
Plan time: 0.23378
==== Step 40 ====
True state: tiger-right
Belief: [(State(tiger-right), 1.0)]
Action: stay
Reward: -1
>> Observation: tiger-right
Num sims: 1000
Plan time: 0.28065
==== Step 41 ====
True state: tiger-right
Belief: [(State(tiger-right), 1.0)]
Action: listen
Reward: -1
>> Observation: tiger-right
Num sims: 1000
Plan time: 0.22209
==== Step 42 ====
True state: tiger-right
Belief: [(State(tiger-right), 1.0)]
Action: listen
Reward: -1
>> Observation: tiger-right
Num sims: 1000
Plan time: 0.21282
==== Step 43 ====
True state: tiger-right
Belief: [(State(tiger-right), 1.0)]
Action: stay
Reward: -1
>> Observation: tiger-right
Num sims: 1000
Plan time: 0.39765
==== Step 44 ====
True state: tiger-right
Belief: [(State(tiger-right), 1.0)]
Action: stay
Reward: -1
>> Observation: tiger-right
Num sims: 1000
Plan time: 0.32241
==== Step 45 ====
True state: tiger-right
Belief: [(State(tiger-right), 1.0)]
Action: listen
Reward: -1
>> Observation: tiger-right
Num sims: 1000
Plan time: 0.39873
==== Step 46 ====
True state: tiger-right
Belief: [(State(tiger-right), 1.0)]
Action: listen
Reward: -1
>> Observation: tiger-right
Num sims: 1000
Plan time: 0.22672
==== Step 47 ====
True state: tiger-right
Belief: [(State(tiger-right), 1.0)]
Action: stay
Reward: -1
>> Observation: tiger-right
Num sims: 1000
Plan time: 0.23397
==== Step 48 ====
True state: tiger-right
Belief: [(State(tiger-right), 1.0)]
Action: stay
Reward: -1
>> Observation: tiger-right
Num sims: 1000
Plan time: 0.20136
==== Step 49 ====
True state: tiger-right
Belief: [(State(tiger-right), 1.0)]
Action: listen
Reward: -1
>> Observation: tiger-right
Num sims: 1000
Plan time: 0.24157
==== Step 50 ====
True state: tiger-right
Belief: [(State(tiger-right), 1.0)]
Action: open-left
Reward: 10
>> Observation: tiger-right
Num sims: 1000
Plan time: 0.19566


==== Step 51 ====
True state: tiger-right
Belief: [(State(tiger-right), 0.5133333333333333), (State(tiger-left), 0.4866666666666667)]
Action: listen
Reward: -1
>> Observation: tiger-right
Num sims: 1000
Plan time: 0.23579
==== Step 52 ====
True state: tiger-right
Belief: [(State(tiger-right), 0.8565121412803532), (State(tiger-left), 0.1434878587196468)]
Action: listen
Reward: -1
>> Observation: tiger-right
Num sims: 1000
Plan time: 0.22476
==== Step 53 ====
True state: tiger-right
Belief: [(State(tiger-right), 0.9746835443037974), (State(tiger-left), 0.02531645569620253)]
Action: stay
Reward: -1
>> Observation: tiger-right
Num sims: 1000
Plan time: 0.27496
==== Step 54 ====
True state: tiger-right
Belief: [(State(tiger-right), 0.981675392670157), (State(tiger-left), 0.01832460732984293)]
Action: listen
Reward: -1
>> Observation: tiger-right
Num sims: 1000
Plan time: 0.22312
==== Step 55 ====
True state: tiger-right
Belief: [(State(tiger-right), 0.9897209985315712), (State(tiger-left), 0.010279001468428781)]
Action: listen
Reward: -1
>> Observation: tiger-right
Num sims: 1000
Plan time: 0.21919
==== Step 56 ====
True state: tiger-right
Belief: [(State(tiger-right), 1.0)]
Action: stay
Reward: -1
>> Observation: tiger-right
Num sims: 1000
Plan time: 0.28927
==== Step 57 ====
True state: tiger-right
Belief: [(State(tiger-right), 1.0)]
Action: listen
Reward: -1
>> Observation: tiger-right
Num sims: 1000
Plan time: 0.20447
==== Step 58 ====
True state: tiger-right
Belief: [(State(tiger-right), 1.0)]
Action: stay
Reward: -1
>> Observation: tiger-right
Num sims: 1000
Plan time: 0.18842
==== Step 59 ====
True state: tiger-right
Belief: [(State(tiger-right), 1.0)]
Action: listen
Reward: -1
>> Observation: tiger-right
Num sims: 1000
Plan time: 0.27513
==== Step 60 ====
True state: tiger-right
Belief: [(State(tiger-right), 1.0)]
Action: listen
Reward: -1
>> Observation: tiger-right
Num sims: 1000
Plan time: 0.30055
==== Step 61 ====
True state: tiger-right
Belief: [(State(tiger-right), 1.0)]
Action: stay
Reward: -1
>> Observation: tiger-right
Num sims: 1000
Plan time: 0.27954
==== Step 62 ====
True state: tiger-right
Belief: [(State(tiger-right), 1.0)]
Action: listen
Reward: -1
>> Observation: tiger-right
Num sims: 1000
Plan time: 0.22149
==== Step 63 ====
True state: tiger-right
Belief: [(State(tiger-right), 1.0)]
Action: listen
Reward: -1
>> Observation: tiger-right
Num sims: 1000
Plan time: 0.25805
==== Step 64 ====
True state: tiger-right
Belief: [(State(tiger-right), 1.0)]
Action: open-left
Reward: 10
>> Observation: tiger-right
Num sims: 1000
Plan time: 0.26581


==== Step 65 ====
True state: tiger-right
Belief: [(State(tiger-left), 0.5363984674329502), (State(tiger-right), 0.46360153256704983)]
Action: listen
Reward: -1
>> Observation: tiger-right
Num sims: 1000
Plan time: 0.21913
==== Step 66 ====
True state: tiger-right
Belief: [(State(tiger-right), 0.8526315789473684), (State(tiger-left), 0.14736842105263157)]
Action: listen
Reward: -1
>> Observation: tiger-right
Num sims: 1000
Plan time: 0.21123
==== Step 67 ====
True state: tiger-right
Belief: [(State(tiger-right), 0.9656946826758147), (State(tiger-left), 0.03430531732418525)]
Action: open-left
Reward: 10
>> Observation: tiger-right
Num sims: 1000
Plan time: 0.22831


==== Step 68 ====
True state: tiger-right
Belief: [(State(tiger-left), 0.5102040816326531), (State(tiger-right), 0.4897959183673469)]
Action: listen
Reward: -1
>> Observation: tiger-right
Num sims: 1000
Plan time: 0.36176
==== Step 69 ====
True state: tiger-right
Belief: [(State(tiger-right), 0.8461538461538461), (State(tiger-left), 0.15384615384615385)]
Action: stay
Reward: -1
>> Observation: tiger-right
Num sims: 1000
Plan time: 0.30878
==== Step 70 ====
True state: tiger-right
Belief: [(State(tiger-right), 0.8765432098765432), (State(tiger-left), 0.12345679012345678)]
Action: listen
Reward: -1
>> Observation: tiger-right
Num sims: 1000
Plan time: 0.18535
==== Step 71 ====
True state: tiger-right
Belief: [(State(tiger-right), 0.9756521739130435), (State(tiger-left), 0.02434782608695652)]
Action: stay
Reward: -1
>> Observation: tiger-right
Num sims: 1000
Plan time: 0.23655
==== Step 72 ====
True state: tiger-right
Belief: [(State(tiger-right), 0.9855595667870036), (State(tiger-left), 0.01444043321299639)]
Action: listen
Reward: -1
>> Observation: tiger-right
Num sims: 1000
Plan time: 0.26056
==== Step 73 ====
True state: tiger-right
Belief: [(State(tiger-right), 0.9952038369304557), (State(tiger-left), 0.004796163069544364)]
Action: stay
Reward: -1
>> Observation: tiger-right
Num sims: 1000
Plan time: 0.17916
==== Step 74 ====
True state: tiger-right
Belief: [(State(tiger-right), 0.9896907216494846), (State(tiger-left), 0.010309278350515464)]
Action: listen
Reward: -1
>> Observation: tiger-right
Num sims: 1000
Plan time: 0.19693
==== Step 75 ====
True state: tiger-right
Belief: [(State(tiger-right), 0.998158379373849), (State(tiger-left), 0.001841620626151013)]
Action: listen
Reward: -1
>> Observation: tiger-right
Num sims: 1000
Plan time: 0.33473
==== Step 76 ====
True state: tiger-right
Belief: [(State(tiger-right), 1.0)]
Action: listen
Reward: -1
>> Observation: tiger-right
Num sims: 1000
Plan time: 0.26001
==== Step 77 ====
True state: tiger-right
Belief: [(State(tiger-right), 1.0)]
Action: listen
Reward: -1
>> Observation: tiger-right
Num sims: 1000
Plan time: 0.25870
==== Step 78 ====
True state: tiger-right
Belief: [(State(tiger-right), 1.0)]
Action: listen
Reward: -1
>> Observation: tiger-right
Num sims: 1000
Plan time: 0.25702
==== Step 79 ====
True state: tiger-right
Belief: [(State(tiger-right), 1.0)]
Action: stay
Reward: -1
>> Observation: tiger-right
Num sims: 1000
Plan time: 0.29869
==== Step 80 ====
True state: tiger-right
Belief: [(State(tiger-right), 1.0)]
Action: stay
Reward: -1
>> Observation: tiger-right
Num sims: 1000
Plan time: 0.21366
==== Step 81 ====
True state: tiger-right
Belief: [(State(tiger-right), 1.0)]
Action: listen
Reward: -1
>> Observation: tiger-right
Num sims: 1000
Plan time: 0.18700
==== Step 82 ====
True state: tiger-right
Belief: [(State(tiger-right), 1.0)]
Action: stay
Reward: -1
>> Observation: tiger-right
Num sims: 1000
Plan time: 0.34132
==== Step 83 ====
True state: tiger-right
Belief: [(State(tiger-right), 1.0)]
Action: listen
Reward: -1
>> Observation: tiger-right
Num sims: 1000
Plan time: 0.30739
==== Step 84 ====
True state: tiger-right
Belief: [(State(tiger-right), 1.0)]
Action: listen
Reward: -1
>> Observation: tiger-right
Num sims: 1000
Plan time: 0.23322
==== Step 85 ====
True state: tiger-right
Belief: [(State(tiger-right), 1.0)]
Action: listen
Reward: -1
>> Observation: tiger-right
Num sims: 1000
Plan time: 0.27540
==== Step 86 ====
True state: tiger-right
Belief: [(State(tiger-right), 1.0)]
Action: listen
Reward: -1
>> Observation: tiger-right
Num sims: 1000
Plan time: 0.24111
==== Step 87 ====
True state: tiger-right
Belief: [(State(tiger-right), 1.0)]
Action: listen
Reward: -1
>> Observation: tiger-right
Num sims: 1000
Plan time: 0.24698
==== Step 88 ====
True state: tiger-right
Belief: [(State(tiger-right), 1.0)]
Action: stay
Reward: -1
>> Observation: tiger-right
Num sims: 1000
Plan time: 0.18925
==== Step 89 ====
True state: tiger-right
Belief: [(State(tiger-right), 1.0)]
Action: stay
Reward: -1
>> Observation: tiger-right
Num sims: 1000
Plan time: 0.27992
==== Step 90 ====
True state: tiger-right
Belief: [(State(tiger-right), 1.0)]
Action: stay
Reward: -1
>> Observation: tiger-right
Num sims: 1000
Plan time: 0.20685
==== Step 91 ====
True state: tiger-right
Belief: [(State(tiger-right), 1.0)]
Action: stay
Reward: -1
>> Observation: tiger-right
Num sims: 1000
Plan time: 0.23234
==== Step 92 ====
True state: tiger-right
Belief: [(State(tiger-right), 1.0)]
Action: stay
Reward: -1
>> Observation: tiger-right
Num sims: 1000
Plan time: 0.19272
==== Step 93 ====
True state: tiger-right
Belief: [(State(tiger-right), 1.0)]
Action: open-left
Reward: 10
>> Observation: tiger-right
Num sims: 1000
Plan time: 0.27103


==== Step 94 ====
True state: tiger-right
Belief: [(State(tiger-left), 0.5174603174603175), (State(tiger-right), 0.48253968253968255)]
Action: listen
Reward: -1
>> Observation: tiger-right
Num sims: 1000
Plan time: 0.23416
==== Step 95 ====
True state: tiger-right
Belief: [(State(tiger-right), 0.8114942528735632), (State(tiger-left), 0.18850574712643678)]
Action: listen
Reward: -1
>> Observation: tiger-right
Num sims: 1000
Plan time: 0.22160
==== Step 96 ====
True state: tiger-right
Belief: [(State(tiger-right), 0.9630872483221476), (State(tiger-left), 0.03691275167785235)]
Action: stay
Reward: -1
>> Observation: tiger-right
Num sims: 1000
Plan time: 0.23164
==== Step 97 ====
True state: tiger-right
Belief: [(State(tiger-right), 0.9502923976608187), (State(tiger-left), 0.049707602339181284)]
Action: listen
Reward: -1
>> Observation: tiger-right
Num sims: 1000
Plan time: 0.19868
==== Step 98 ====
True state: tiger-right
Belief: [(State(tiger-right), 0.9934924078091106), (State(tiger-left), 0.006507592190889371)]
Action: listen
Reward: -1
>> Observation: tiger-right
Num sims: 1000
Plan time: 0.27353
==== Step 99 ====
True state: tiger-right
Belief: [(State(tiger-right), 1.0)]
Action: open-left
Reward: 10
>> Observation: tiger-right
Num sims: 1000
Plan time: 0.19624


==== Step 100 ====
True state: tiger-right
Belief: [(State(tiger-left), 0.5294117647058824), (State(tiger-right), 0.47058823529411764)]
Action: listen
Reward: -1
>> Observation: tiger-right
Num sims: 1000
Plan time: 0.29093
